{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open(\"sample.html\", \"r\", encoding=\"utf-8\") as file:\n",
    "    content = file.read()\n",
    "\n",
    "soup = BeautifulSoup(content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\birap\\Downloads\\2025-04-04_-_Worker_and_Temporary_Worker.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Organisation Name</th>\n",
       "      <th>Town/City</th>\n",
       "      <th>County</th>\n",
       "      <th>Type &amp; Rating</th>\n",
       "      <th>Route</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>McMullan Shellfish</td>\n",
       "      <td>Ballymena</td>\n",
       "      <td>Co Antrim</td>\n",
       "      <td>Worker (A rating)</td>\n",
       "      <td>Skilled Worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(A1F1 Limited T/A ) Ultrasound Direct London</td>\n",
       "      <td>Croydon</td>\n",
       "      <td>London</td>\n",
       "      <td>Worker (A rating)</td>\n",
       "      <td>Skilled Worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(IECC Care) Independent Excel Care Consortium ...</td>\n",
       "      <td>Colchester</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Worker (A rating)</td>\n",
       "      <td>Skilled Worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>*ABOUTCARE HASTINGS LTD</td>\n",
       "      <td>East Sussex</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Worker (A rating)</td>\n",
       "      <td>Skilled Worker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>.LITTLE NOORIYAH LTD</td>\n",
       "      <td>Smethwick</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Worker (A rating)</td>\n",
       "      <td>Skilled Worker</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   Organisation Name    Town/City     County  \\\n",
       "0                                 McMullan Shellfish    Ballymena  Co Antrim   \n",
       "1       (A1F1 Limited T/A ) Ultrasound Direct London      Croydon     London   \n",
       "2  (IECC Care) Independent Excel Care Consortium ...   Colchester        NaN   \n",
       "3                            *ABOUTCARE HASTINGS LTD  East Sussex        NaN   \n",
       "4                               .LITTLE NOORIYAH LTD    Smethwick        NaN   \n",
       "\n",
       "       Type & Rating           Route  \n",
       "0  Worker (A rating)  Skilled Worker  \n",
       "1  Worker (A rating)  Skilled Worker  \n",
       "2  Worker (A rating)  Skilled Worker  \n",
       "3  Worker (A rating)  Skilled Worker  \n",
       "4  Worker (A rating)  Skilled Worker  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_list = list(df['Organisation Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_cards = soup.select(\"ol#searchResults li.results__item\")\n",
    "\n",
    "obs_on_page = []\n",
    "for job in job_cards:\n",
    "    section_element = job.find(\"article\")\n",
    "    title = section_element.get(\"data-job-title\", \"\")\n",
    "    company = section_element.get(\"data-company-name\", \"\")\n",
    "    location = section_element.get(\"data-job-location\", \"\")\n",
    "    salary = section_element.get(\"data-job-salary\", \"\")\n",
    "    job_type = section_element.get(\"data-job-type\", \"\")\n",
    "    date_posted = section_element.get(\"data-job-posted\", \"\")\n",
    "    job_id = section_element.get(\"data-job-id\", \"\")\n",
    "    job_url = f'https://www.cv-library.co.uk/job/{job_id}' if job_id else \"\"\n",
    "\n",
    "    company_logo_element = job.select_one(\"img.job__logo\")\n",
    "    company_logo = company_logo_element.get(\"src\", \"\") if company_logo_element else \"\"\n",
    "\n",
    "    if company in company_list:\n",
    "        obs_on_page.append({\n",
    "                        \"Title\": title,\n",
    "                        \"Company\": company,\n",
    "                        \"Location\": location,\n",
    "                        \"Salary\": salary,\n",
    "                        \"Job Type\": job_type,\n",
    "                        \"URL\": job_url,\n",
    "                        \"Date Posted\": date_posted,\n",
    "                        \"Company Logo\": company_logo\n",
    "                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Title': 'Lead Traffic Signal Engineer',\n",
       "  'Company': 'Yunex Limited',\n",
       "  'Location': 'East Midlands',\n",
       "  'Salary': '',\n",
       "  'Job Type': '',\n",
       "  'URL': 'https://www.cv-library.co.uk/job/223298739',\n",
       "  'Date Posted': '',\n",
       "  'Company Logo': '/assets/images/placeholder-600x400-fdfdfd-ed7103c8d6c9f61d0cbb5e65611c40b8f2381a9233a1828193af8aa84a24f1cf.jpg'},\n",
       " {'Title': 'Healthcare Assistant - Deaf Service',\n",
       "  'Company': 'Elysium Healthcare',\n",
       "  'Location': 'OL8 4EF',\n",
       "  'Salary': '£25,058/yearly',\n",
       "  'Job Type': 'Permanent',\n",
       "  'URL': 'https://www.cv-library.co.uk/job/223306830',\n",
       "  'Date Posted': '2025-04-04T13:05:25Z',\n",
       "  'Company Logo': ''},\n",
       " {'Title': 'Senior Social Worker',\n",
       "  'Company': 'Elysium Healthcare',\n",
       "  'Location': 'WD6 1JN',\n",
       "  'Salary': '£42,372/yearly',\n",
       "  'Job Type': 'Permanent',\n",
       "  'URL': 'https://www.cv-library.co.uk/job/223306829',\n",
       "  'Date Posted': '2025-04-04T13:05:25Z',\n",
       "  'Company Logo': ''},\n",
       " {'Title': 'Supporter Services and Ticketing Admin Apprentice',\n",
       "  'Company': 'Brighton & Hove Albion Football Club',\n",
       "  'Location': 'Brighton',\n",
       "  'Salary': '£7.55/hour',\n",
       "  'Job Type': 'Permanent',\n",
       "  'URL': 'https://www.cv-library.co.uk/job/223306804',\n",
       "  'Date Posted': '2025-04-04T13:02:25Z',\n",
       "  'Company Logo': ''},\n",
       " {'Title': 'Forklift Engineer',\n",
       "  'Company': 'M4 Recruitment',\n",
       "  'Location': 'RG7, Ufton Nervet, West Berkshire',\n",
       "  'Salary': '£35,000 - £40,000/annum',\n",
       "  'Job Type': 'Permanent',\n",
       "  'URL': 'https://www.cv-library.co.uk/job/223008680',\n",
       "  'Date Posted': '2025-04-04T12:59:58Z',\n",
       "  'Company Logo': ''},\n",
       " {'Title': 'Sales Executive',\n",
       "  'Company': 'M4 Recruitment',\n",
       "  'Location': 'Theale, West Berkshire',\n",
       "  'Salary': '£24,000 - £26,000/annum Commission structure ',\n",
       "  'Job Type': 'Permanent',\n",
       "  'URL': 'https://www.cv-library.co.uk/job/223117848',\n",
       "  'Date Posted': '2025-04-04T12:59:58Z',\n",
       "  'Company Logo': ''},\n",
       " {'Title': 'Hvac Engineer',\n",
       "  'Company': 'M4 Recruitment',\n",
       "  'Location': 'RG7, Ufton Nervet, West Berkshire',\n",
       "  'Salary': '£40,000 - £47,000/annum',\n",
       "  'Job Type': 'Permanent',\n",
       "  'URL': 'https://www.cv-library.co.uk/job/223126045',\n",
       "  'Date Posted': '2025-04-04T12:59:57Z',\n",
       "  'Company Logo': ''},\n",
       " {'Title': 'Fire Alarm Engineer',\n",
       "  'Company': 'Total Integrated Solutions Limited',\n",
       "  'Location': 'NG18, Mansfield, Nottinghamshire',\n",
       "  'Salary': '£38,000 - £41,000/annum',\n",
       "  'Job Type': 'Permanent',\n",
       "  'URL': 'https://www.cv-library.co.uk/job/223306709',\n",
       "  'Date Posted': '2025-04-04T12:54:14Z',\n",
       "  'Company Logo': ''}]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_on_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XML file 'jobs.xml' created successfully!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Load the JSON data\n",
    "with open(\"glassdoor_all_uk.json\", \"r\", encoding=\"utf-8\") as file:\n",
    "    jobs = json.load(file)\n",
    "\n",
    "# Create root element\n",
    "root = ET.Element(\"Jobs\")\n",
    "\n",
    "for job in jobs:\n",
    "    job_element = ET.SubElement(root, \"Job\")\n",
    "    \n",
    "    for key, value in job.items():\n",
    "        # Create a sub-element for each field (Title, Company, etc.)\n",
    "        child = ET.SubElement(job_element, key.replace(\" \", \"\"))\n",
    "        child.text = value if value else \"\"\n",
    "\n",
    "# Create the XML tree and write to file\n",
    "tree = ET.ElementTree(root)\n",
    "tree.write(\"glassdoor_all_uk.json.xml\", encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "print(\"XML file 'jobs.xml' created successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1417725782.py, line 1)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mpip3 install https://github.com/tomquirk/linkedin-api.git\u001b[39m\n         ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "pip3 install https://github.com/tomquirk/linkedin-api.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting linkedin-api\n",
      "  Downloading linkedin_api-2.3.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.12.3 in c:\\users\\birap\\projects\\freelancing\\job_portal_extractor\\venv\\lib\\site-packages (from linkedin-api) (4.13.3)\n",
      "Collecting lxml<6.0.0,>=5.3.0 (from linkedin-api)\n",
      "  Downloading lxml-5.3.2-cp312-cp312-win_amd64.whl.metadata (3.7 kB)\n",
      "Collecting requests<3.0.0,>=2.32.3 (from linkedin-api)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\birap\\projects\\freelancing\\job_portal_extractor\\venv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->linkedin-api) (2.6)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\birap\\projects\\freelancing\\job_portal_extractor\\venv\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.12.3->linkedin-api) (4.13.1)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.32.3->linkedin-api)\n",
      "  Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.32.3->linkedin-api)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.32.3->linkedin-api)\n",
      "  Using cached urllib3-2.4.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.32.3->linkedin-api)\n",
      "  Using cached certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Downloading linkedin_api-2.3.1-py3-none-any.whl (26 kB)\n",
      "Downloading lxml-5.3.2-cp312-cp312-win_amd64.whl (3.8 MB)\n",
      "   ---------------------------------------- 0.0/3.8 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.1/3.8 MB 2.4 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.6/3.8 MB 5.8 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 1.0/3.8 MB 7.0 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 1.5/3.8 MB 7.9 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 2.1/3.8 MB 8.8 MB/s eta 0:00:01\n",
      "   -------------------------- ------------- 2.6/3.8 MB 9.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 3.0/3.8 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 3.6/3.8 MB 9.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 3.8/3.8 MB 9.4 MB/s eta 0:00:00\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp312-cp312-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.4.0-py3-none-any.whl (128 kB)\n",
      "Installing collected packages: urllib3, lxml, idna, charset-normalizer, certifi, requests, linkedin-api\n",
      "Successfully installed certifi-2025.1.31 charset-normalizer-3.4.1 idna-3.10 linkedin-api-2.3.1 lxml-5.3.2 requests-2.32.3 urllib3-2.4.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install linkedin-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinkedinSessionExpired",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLinkedinSessionExpired\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlinkedin_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Linkedin\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m api = \u001b[43mLinkedin\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbirappa.001@gmail.com\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mShreya@500\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\birap\\projects\\freelancing\\job_portal_extractor\\venv\\Lib\\site-packages\\linkedin_api\\linkedin.py:82\u001b[39m, in \u001b[36mLinkedin.__init__\u001b[39m\u001b[34m(self, username, password, authenticate, refresh_cookies, debug, proxies, cookies, cookies_dir)\u001b[39m\n\u001b[32m     80\u001b[39m     \u001b[38;5;28mself\u001b[39m.client._set_session_cookies(cookies)\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mauthenticate\u001b[49m\u001b[43m(\u001b[49m\u001b[43musername\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\birap\\projects\\freelancing\\job_portal_extractor\\venv\\Lib\\site-packages\\linkedin_api\\client.py:94\u001b[39m, in \u001b[36mClient.authenticate\u001b[39m\u001b[34m(self, username, password)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._use_cookie_cache:\n\u001b[32m     93\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAttempting to use cached cookies\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m     cookies = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cookie_repository\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43musername\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m cookies:\n\u001b[32m     96\u001b[39m         \u001b[38;5;28mself\u001b[39m.logger.debug(\u001b[33m\"\u001b[39m\u001b[33mUsing cached cookies\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\birap\\projects\\freelancing\\job_portal_extractor\\venv\\Lib\\site-packages\\linkedin_api\\cookie_repository.py:38\u001b[39m, in \u001b[36mCookieRepository.get\u001b[39m\u001b[34m(self, username)\u001b[39m\n\u001b[32m     36\u001b[39m cookies = \u001b[38;5;28mself\u001b[39m._load_cookies_from_cache(username)\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cookies \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m CookieRepository._is_token_still_valid(cookies):\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LinkedinSessionExpired\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cookies\n",
      "\u001b[31mLinkedinSessionExpired\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from linkedin_api import Linkedin\n",
    "\n",
    "api = Linkedin(\"birappa.001@gmail.com\", \"Shreya@500\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def get_chat_id(TOKEN: str) -> str:\n",
    "    '''Get the chatid for our telegram bot'''\n",
    "    \n",
    "    url = f\"https://api.telegram.org/bot{TOKEN}/getUpdates\"\n",
    "    response = requests.get(url)\n",
    "    info = response.json()\n",
    "    print(info)\n",
    "    chat_id = info['result'][0]['message']['chat']['id']\n",
    "\n",
    "    return chat_id\n",
    "\n",
    "def send_message(TOKEN: str, message: str, chat_id: str):\n",
    "    '''Send failed notification to bot'''\n",
    "\n",
    "    url = f\"https://api.telegram.org/bot{TOKEN}/sendMessage?chat_id={chat_id}&text={message}\"\n",
    "\n",
    "    response = requests.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ok': True, 'result': [{'update_id': 257458296, 'message': {'message_id': 33, 'from': {'id': 2142797509, 'is_bot': False, 'first_name': 'Chozi', 'username': 'Birappa_G', 'language_code': 'en'}, 'chat': {'id': 2142797509, 'first_name': 'Chozi', 'username': 'Birappa_G', 'type': 'private'}, 'date': 1744633811, 'text': '/start', 'entities': [{'offset': 0, 'length': 6, 'type': 'bot_command'}]}}, {'update_id': 257458297, 'message': {'message_id': 34, 'from': {'id': 2142797509, 'is_bot': False, 'first_name': 'Chozi', 'username': 'Birappa_G', 'language_code': 'en'}, 'chat': {'id': 2142797509, 'first_name': 'Chozi', 'username': 'Birappa_G', 'type': 'private'}, 'date': 1744633819, 'text': 'Remember to subscribe'}}]}\n"
     ]
    }
   ],
   "source": [
    "TOKEN = '7844666863:AAF0fTu1EqWC1v55oC25TVzSjClSuxkO2X4'\n",
    "\n",
    "chat_id = get_chat_id(TOKEN)\n",
    "send_message(TOKEN, \"Hello\", chat_id) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting linkedin-jobs-scraper\n",
      "  Downloading linkedin_jobs_scraper-5.0.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting selenium>=4.12.0 (from linkedin-jobs-scraper)\n",
      "  Downloading selenium-4.31.0-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in c:\\users\\birap\\projects\\freelancing\\job_portal_extractor\\venv\\lib\\site-packages (from urllib3[socks]<3,>=1.26->selenium>=4.12.0->linkedin-jobs-scraper) (2.4.0)\n",
      "Collecting trio~=0.17 (from selenium>=4.12.0->linkedin-jobs-scraper)\n",
      "  Downloading trio-0.29.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting trio-websocket~=0.9 (from selenium>=4.12.0->linkedin-jobs-scraper)\n",
      "  Downloading trio_websocket-0.12.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\birap\\projects\\freelancing\\job_portal_extractor\\venv\\lib\\site-packages (from selenium>=4.12.0->linkedin-jobs-scraper) (2025.1.31)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in c:\\users\\birap\\projects\\freelancing\\job_portal_extractor\\venv\\lib\\site-packages (from selenium>=4.12.0->linkedin-jobs-scraper) (4.13.1)\n",
      "Collecting websocket-client~=1.8 (from selenium>=4.12.0->linkedin-jobs-scraper)\n",
      "  Using cached websocket_client-1.8.0-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting attrs>=23.2.0 (from trio~=0.17->selenium>=4.12.0->linkedin-jobs-scraper)\n",
      "  Downloading attrs-25.3.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting sortedcontainers (from trio~=0.17->selenium>=4.12.0->linkedin-jobs-scraper)\n",
      "  Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: idna in c:\\users\\birap\\projects\\freelancing\\job_portal_extractor\\venv\\lib\\site-packages (from trio~=0.17->selenium>=4.12.0->linkedin-jobs-scraper) (3.10)\n",
      "Collecting outcome (from trio~=0.17->selenium>=4.12.0->linkedin-jobs-scraper)\n",
      "  Using cached outcome-1.3.0.post0-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting sniffio>=1.3.0 (from trio~=0.17->selenium>=4.12.0->linkedin-jobs-scraper)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting cffi>=1.14 (from trio~=0.17->selenium>=4.12.0->linkedin-jobs-scraper)\n",
      "  Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl.metadata (1.6 kB)\n",
      "Collecting wsproto>=0.14 (from trio-websocket~=0.9->selenium>=4.12.0->linkedin-jobs-scraper)\n",
      "  Using cached wsproto-1.2.0-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pysocks!=1.5.7,<2.0,>=1.5.6 (from urllib3[socks]<3,>=1.26->selenium>=4.12.0->linkedin-jobs-scraper)\n",
      "  Using cached PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pycparser (from cffi>=1.14->trio~=0.17->selenium>=4.12.0->linkedin-jobs-scraper)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Collecting h11<1,>=0.9.0 (from wsproto>=0.14->trio-websocket~=0.9->selenium>=4.12.0->linkedin-jobs-scraper)\n",
      "  Using cached h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Downloading linkedin_jobs_scraper-5.0.2-py3-none-any.whl (28 kB)\n",
      "Downloading selenium-4.31.0-py3-none-any.whl (9.4 MB)\n",
      "   ---------------------------------------- 0.0/9.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.2/9.4 MB 4.6 MB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.8/9.4 MB 8.2 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.2/9.4 MB 9.7 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 1.8/9.4 MB 9.7 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.3/9.4 MB 9.6 MB/s eta 0:00:01\n",
      "   ------------ --------------------------- 2.9/9.4 MB 10.1 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 3.4/9.4 MB 10.2 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 3.9/9.4 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 4.4/9.4 MB 10.8 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 4.9/9.4 MB 10.9 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 5.4/9.4 MB 10.8 MB/s eta 0:00:01\n",
      "   ------------------------ --------------- 5.8/9.4 MB 10.6 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.3/9.4 MB 10.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.8/9.4 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.3/9.4 MB 10.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.7/9.4 MB 10.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.2/9.4 MB 10.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.7/9.4 MB 10.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  9.2/9.4 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.4/9.4 MB 10.3 MB/s eta 0:00:00\n",
      "Downloading trio-0.29.0-py3-none-any.whl (492 kB)\n",
      "   ---------------------------------------- 0.0/492.9 kB ? eta -:--:--\n",
      "   ------------------------------------ --- 450.6/492.9 kB 9.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 492.9/492.9 kB 7.7 MB/s eta 0:00:00\n",
      "Downloading trio_websocket-0.12.2-py3-none-any.whl (21 kB)\n",
      "Using cached websocket_client-1.8.0-py3-none-any.whl (58 kB)\n",
      "Downloading attrs-25.3.0-py3-none-any.whl (63 kB)\n",
      "   ---------------------------------------- 0.0/63.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 63.8/63.8 kB ? eta 0:00:00\n",
      "Using cached cffi-1.17.1-cp312-cp312-win_amd64.whl (181 kB)\n",
      "Using cached outcome-1.3.0.post0-py2.py3-none-any.whl (10 kB)\n",
      "Using cached PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Using cached sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Using cached h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Installing collected packages: sortedcontainers, websocket-client, sniffio, pysocks, pycparser, h11, attrs, wsproto, outcome, cffi, trio, trio-websocket, selenium, linkedin-jobs-scraper\n",
      "Successfully installed attrs-25.3.0 cffi-1.17.1 h11-0.14.0 linkedin-jobs-scraper-5.0.2 outcome-1.3.0.post0 pycparser-2.22 pysocks-1.7.1 selenium-4.31.0 sniffio-1.3.1 sortedcontainers-2.4.0 trio-0.29.0 trio-websocket-0.12.2 websocket-client-1.8.0 wsproto-1.2.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install linkedin-jobs-scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkedin_jobs_scraper.query import Query, QueryOptions, QueryFilters\n",
    "from linkedin_jobs_scraper.filters import RelevanceFilters, TimeFilters, TypeFilters, ExperienceLevelFilters, \\\n",
    "    OnSiteOrRemoteFilters, IndustryFilters, SalaryBaseFilters\n",
    "    \n",
    "query = Query(\n",
    "    query='Engineer',\n",
    "    options=QueryOptions(\n",
    "        locations=['United Kingdom'],        \n",
    "        apply_link=True,\n",
    "        skip_promoted_jobs=True,\n",
    "        limit=10000,\n",
    "        filters=QueryFilters(\n",
    "            relevance=RelevanceFilters.RECENT,\n",
    "            time=TimeFilters.MONTH,\n",
    "            type=[TypeFilters.FULL_TIME],\n",
    "            experience=[ExperienceLevelFilters.MID_SENIOR],\n",
    "            base_salary=SalaryBaseFilters.SALARY_100K\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(\"AnonymousStrategy is no longer maintained and it won't probably work. It is recommended to use an authenticated session, see documentation at https://github.com/spinlud/py-linkedin-jobs-scraper#anonymous-vs-authenticated-session.\",)\n"
     ]
    }
   ],
   "source": [
    "from linkedin_jobs_scraper import LinkedinScraper\n",
    "\n",
    "scraper = LinkedinScraper(\n",
    "    chrome_executable_path=None,  # Custom Chrome executable path (e.g. /foo/bar/bin/chromedriver)\n",
    "    chrome_binary_location=None,  # Custom path to Chrome/Chromium binary (e.g. /foo/bar/chrome-mac/Chromium.app/Contents/MacOS/Chromium)\n",
    "    chrome_options=None,  # Custom Chrome options here\n",
    "    headless=False,  # Overrides headless mode only if chrome_options is None\n",
    "    max_workers=1,  # How many threads will be spawned to run queries concurrently (one Chrome driver for each thread)\n",
    "    slow_mo=0.5,  # Slow down the scraper to avoid 'Too many requests 429' errors (in seconds)\n",
    "    page_load_timeout=40  # Page load timeout (in seconds)    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkedin_jobs_scraper.events import Events, EventData, EventMetrics\n",
    "\n",
    "def on_data(data: EventData):\n",
    "    print(data.title, data.company, data.company_link, data.date, data.date_text, data.link, data.insights, data.description)\n",
    "\n",
    "\n",
    "# Fired once for each page (25 jobs)\n",
    "def on_metrics(metrics: EventMetrics):\n",
    "    print('[ON_METRICS]', str(metrics))\n",
    "\n",
    "\n",
    "def on_error(error):\n",
    "    print('[ON_ERROR]', error)\n",
    "\n",
    "\n",
    "def on_end():\n",
    "    print('[ON_END]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add event listeners\n",
    "scraper.on(Events.DATA, on_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "scraper.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
